---
layout:     post
title:      "New Paper"
subtitle:   "Activity Identification and Local Linear Convergence of Douglas-Rachford/ADMM under Partial Smoothness"
date:       2014-12-26 12:00:00
author:     "Gabriel Peyr√©"
header-img: "../img/hokusai-0.jpg"
---


[Jingwei Liang](https://www.greyc.fr/users/jliang), [Jalal Fadili](https://fadili.users.greyc.fr/), [Russell Luke](http://num.math.uni-goettingen.de/~r.luke/) and [myself](http://gpeyre.github.io/) just released [a new paper](http://www.optimization-online.org/DB_HTML/2014/12/4703.html) on the linear convergence rate of the Douglas-Rachford (and the closely related ADMM algorithm) algorithm for the optimization of a sum of so-called partly-smooth functions. This is a follow-up on our previous [NIPS paper](http://arxiv.org/abs/1407.5611) on the linear convergence rate of the forward-backward algorithm. The underlying idea is that DR algorithm identifies in finite time a pair of manifolds (a primal and a dual one), and linear convergence is then achieved because the algorithm takes benefit from the smoothness of the minimized functionals along these two manifolds. This partly-smooth setting is typical of the sparsity/low rank functionals (and more generally low-complexity regularizers) one encounters in imaging sciences and machine learning. For an example of use of this algorithm, you can have a look at this [numerical tour](http://nbviewer.ipython.org/github/gpeyre/numerical-tours/blob/master/matlab/optim_4b_dr.ipynb).
